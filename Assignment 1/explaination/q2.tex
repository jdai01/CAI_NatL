\section{Mutual Information}

\begin{table}[h]
    \centering
    \begin{minipage}{0.45\textwidth} % Adjust the width as needed
        \centering
        \pgfplotstabletypeset[
            col sep=comma,
            columns={w1,w2,pmi}, % Specify columns you want to use
            columns/w1/.style={string type}, % Make the first column a string
            columns/w2/.style={string type}, % Make the second column a string
            columns/pmi/.style={fixed,precision=5}, % Set the 'pmi' column to fixed-point numbers with 4 digits of precision
            header=true, % Display the header
            every head row/.style={before row=\hline, after row=\hline}, % Add horizontal rule under the header
            every last row/.style={after row=\hline}, % Add horizontal rule after the last row
            % column type={|l|l|l}, % Add vertical lines between columns
        ]{output/top_30_rows.csv}
        \caption{30 word pairs with highest pmi value}
        \label{top_30_rows} % Label for referencing
    \end{minipage}%
    \hfill % This adds space between the tables
    \begin{minipage}{0.45\textwidth} % Adjust the width as needed
        \centering
        \pgfplotstabletypeset[
            col sep=comma,
            columns={w1,w2,pmi}, % Specify columns you want to use
            columns/w1/.style={string type}, % Make the first column a string
            columns/w2/.style={string type}, % Make the second column a string
            columns/pmi/.style={fixed,precision=5}, % Set the 'pmi' column to fixed-point numbers with 4 digits of precision
            header=true, % Display the header
            every head row/.style={before row=\hline, after row=\hline}, % Add horizontal rule under the header
            every last row/.style={after row=\hline}, % Add horizontal rule after the last row
            % column type={|l|l|l}, % Add vertical lines between columns
        ]{output/bottom_30_rows.csv}
        \caption{30 word pairs with lowest pmi value}
        \label{bottom_30_rows} % Label for referencing
    \end{minipage}
\end{table}

\subsection*{Observations}

Pointwise mutual information (PMI) is a metric that compares the relative frequency of two outcomes occurring together to the probability of either outcome occurring independently.
A positive PMI value means that the words co-occur more frequently than would be expected, whereas a negative PMI value means they cooccur less frequently than would be expected.
A PMI value of 0 suggests that the words occur independent of each other (occurance by chance).

Many word-pairs with high PMI values (Table \ref{top_30_rows}) are often two-word phrases to convey a certain idea, like ``United States'' and ``tree tops'', 
whereas for word-pairs with low PMI values (Table \ref{bottom_30_rows}) are pairs that either contain ``the'' or ``and'', or phrases that is grammatically incorrect in the English language. 


\subsection*{Discussion of the validity of the independence assumption for unigram models}

A unigram model assumes that each word in a sentence or document is independent of all other words, 
i.e., the probability of a word occurring is independent of the words that came before or after it.
$$P(w_1, w_2, w_3, \dots) = \prod^{n}_{i=1} P(w_i)$$
where $P(w_i)$ is the probability of each word $w_i$.

To assess the validity of the independence assumption, 
the conditional probabilities $P(w_i | w_1, w_2, \dots, w_{i-1})$ (observed) is calculated
and compare to the unigram probability $P(w_i)$ (expected)
using chi-square goodness of fit test.

Chi-square independence test is performed.
\begin{itemize}
    \item $H_0$: Each word in a document is independent of all other words.
    \item $H_1$: Each word in a document is dependent of all other words.
\end{itemize}

\begin{align*}
    \chi^2_{\text{statistics}} & = 572.4405452818747 \\
    \chi^2_{\text{critical}} & = 57009.8446715156 \\
    \chi^2_{\text{statistics}} & < \chi^2_{\text{critical}} & (\text{Fail to reject } H_0)
\end{align*}

As the $H_0$ fails to be rejected, this suggests that observed frequencies (the conditional probabilities $P(w_i | w_1, w_2, \dots, w_{i-1})$) and expected frequencies (unigram probability $P(w_i)$) are the independent, and thus the independence assumption for unigram models is valid.
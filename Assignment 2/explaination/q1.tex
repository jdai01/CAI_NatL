\section{SMS Spam Detection}

Naive Bayes Classifier is used to detect spam in this task \cite{raschka2014naive}.

\subsection{Data Preprocessing Steps}

Steps:
\begin{enumerate}
    \item Tokenisation: breaking down a text corpus into individual tokens
    \item Stop words removal: to remove words that are relatively common and uninformative
    \item Stemming and Lemmatization: 
    \begin{enumerate}
        \item Stemming: the transformation a word into its root form
        \item Lemmatization: to obtain the grammatically correct forms of words
    \end{enumerate}
    % \item N-grams: to group a sequence of $n$-words together. Unigram model is performed here.
    \item Data Splitting: Random 80-20 data split into training and test data
\end{enumerate}




\subsection{Experimental Design and Methods}

\begin{enumerate}
    \item Split the data into training and test data
    \item Building of two different vectorisors based on its features:
    \begin{enumerate}
        \item \textbf{Bag-of-Words} (BoW)
        
        BoW treats each document as a collection of words, disregarding grammar and word order but keeping track of word frequency.
        
        \item \textbf{TF-IDF}
        
        TF-IDF (Term Frequency-Inverse Document Frequency)  is used to evaluate how important a word is to a document in a collection or corpus.
        It helps to weigh terms based on their frequency within a document and their rarity across all documents.

        \begin{align}
            TF(t,d) &= \frac{\# \text{ terms of } t \text{ appears in document } d}{\text{Total } \# \text{ of terms in document }d}\\
            IDF(t) &= \log \left( \frac{\text{Total } \# \text{ of documents}}{\# \text{ of documents containing } t} \right)\\
            TF-IDF(t, d) &= TF(t,d) \cdot IDF(t)
        \end{align}

        \textbf{Term Frequency (TF)} measures how frequently a term occurs in a document.

        \textbf{Inverse Document Frequency (IDF)} measures the importance of a term across all documents in the corpus.

        \textbf{TF-IDF} is the product of TF and IDF. It means that 
        \begin{itemize}
            \item If a term appears frequently in a document but also in many documents, the TF will be high but the IDF will be low, lowering the overall score.
            \item If a term appears frequently in one document but is rare across all documents, the score will be high, emphasizing the importance of that term for the document.
        \end{itemize}

    \end{enumerate}
    \item Trained using Multinomial Naive Bayes form scikit-learn
    \item Compare performance on the same train/test split. 
\end{enumerate}


\subsection{Hyperparameters}

Laplace smoothing is used to find tune the model to obtain the best/optimal evaluation metrics. (Default: $\alpha = 1.0$)
% * n-grams




\subsection{Evaluation Metric \cite{google_accuracy}}

\begin{itemize}
    \item $TP$: A spam message is correctly classified as spam
    \item $TN$: A ham (non-spam) message is correctly classified as ham
    \item $FP$: A ham message is incorrectly classfied as spam
    \item $FN$: A spam message is incorrectly classfied as ham
\end{itemize}


\textbf{Accuracy} is the proportion of all classifications that were correct, whether positive or negative.
\begin{equation}
    \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}

\textbf{Recall} or \textbf{true positive rate (TPR)} is the proportion of all actual positives that were classified correctly as positives.
\begin{equation}
    \text{Recall} = \frac{TP}{TP + FN}
\end{equation}

% \textbf{False positive rate (FPR)} is the proportion of all actual negatives that were classified incorrectly as positives
% \begin{equation}
%     \text{FPR} = \frac{FP}{FP + TN}
% \end{equation}

\textbf{Precision} is the proportion of all the model's positive classifications that are actually positive.
\begin{equation}
    \text{Precision} = \frac{TP}{TP + FP}
\end{equation}

\textbf{$F_\beta$ Score (F1 Score)} provides a balanced measure of a model's precision ($P$) and recall ($R$).
\begin{equation}
    F_\beta = (1 + \beta^2) \frac{P \cdot R}{\beta^2 \cdot P + R}
\end{equation}

% \textbf{Matthews correlation coefficient} is robustness despite imbalanced dataset.
% \begin{equation}
%     MCC = \frac{TP \cdot TN - FP \cdot FN}{\sqrt{(TP + FP) \cdot (TP + FN) \cdot (TN + FP) \cdot (TN + FN)}}
% \end{equation}





\subsection{Findings}

Performance metrics is shown in Figure \ref{fig:performance-metrics}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{output/model_accuracy.png} % replace with your image file
        \caption{Accuracy}
        \label{fig:Accuracy}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{output/model_precision.png} % replace with your image file
        \caption{Precision}
        \label{fig:Precision}
    \end{subfigure}
    
    \vskip\baselineskip % adds vertical space between rows
    
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{output/model_recall.png} % replace with your image file
        \caption{Recall}
        \label{fig:Recall}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{output/model_f1-score.png} % replace with your image file
        \caption{F1 Score}
        \label{fig:f1-score}
    \end{subfigure}
    
    \caption{Performance Metrics between BoW and TF-IDF models}
    \label{fig:performance-metrics}
\end{figure}

\paragraph{Accuracy}
In Figure \ref{fig:Accuracy}, both models have high accuracies, which suggests that both models are able to correctly predict well and accurately the labels for the test data, whether `Ham' or `Spam'.

\paragraph{Precision}
In Figure \ref{fig:Precision}, both models have high precision for both `Ham' and `Spam' labels, which suggests that both models are able to correctly well and  predict precisely the labels for the test data, on both `Ham' and `Spam'.

\paragraph{Recall}
In Figure \ref{fig:Recall}, both models have 1.0 score for recall on `Ham' and `Spam', which suggests that both models were able to correctly identify label `Ham'  and `Spam' test data.
TF-IDF model still have lower recall for `Spam' compared to BoW model, suggesting that it is not as recallative as BoW model. 

\paragraph{F1 Score}
In Figure \ref{fig:f1-score}, both models have high F1-score on both `Ham' and `Spam', which suggests a good balance between precision and recall for `Ham' and `Spam'.

\paragraph{Higher score on `Ham' compared to `Spam'}
There is a higher score on `Ham' than `Spam' is highly due to the imbalanced data of `Ham' and `Spam'.
There is a total of 4516 or approximately 87.4\% `Ham' labelled documents and 653 or approximately 12.6\% `Spam' labelled documents.

\paragraph{Overall better performance of BoW Model}
The BoW model has a higher corresponding metric score compared to TF-IDF model, with slightly better overall identification of `Spam' messages.
The TF-IDF model has a slightly lower precision and lower recall for `Spam' category (which also lowers the F1 score).





\subsection{Drawbacks}

\begin{itemize}
    \item Manual vectorization is slower than optimized libraries.
    \item No deep contextual understanding (can't catch subtle spam phrasing).
\end{itemize}


\subsection{Potential Improvements}

\begin{itemize}
    \item Use higher n-grams for better context (Currently only unigram)
\end{itemize}

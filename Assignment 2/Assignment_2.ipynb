{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "82e7877d",
      "metadata": {
        "id": "82e7877d"
      },
      "source": [
        "# Assignmnet 2 (100 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "297b6d20",
      "metadata": {
        "id": "297b6d20"
      },
      "source": [
        "**Name:** Jiahui Dai<br>\n",
        "**Email:** jid4620@thi.de<br>\n",
        "**Group:** B <br>\n",
        "**Hours spend *(optional)* :** <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ca0db8d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import modules\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import collections\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.stem import PorterStemmer\n",
        "# from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "790c57d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load filenames\n",
        "data_folder = \"sms_spam_collection\"\n",
        "sms_spam_collection = os.path.join(data_folder, \"SMSSpamCollection\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09f79f88",
      "metadata": {
        "id": "09f79f88"
      },
      "source": [
        "### SMS Spam Detection *(60 points)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "148e88d0",
      "metadata": {
        "id": "148e88d0"
      },
      "source": [
        "<p>You are hired as an AI expert in the development department of a telecommunications company. The first thing on your orientation plan is a small project that your boss has assigned you for the following given situation. Your supervisor has given away his private cell phone number on too many websites and is now complaining about daily spam SMS. Therefore, it is your job to write a spam detector in Python. </p>\n",
        "\n",
        "<p>In doing so, you need to use a Naive Bayes classifier that can handle both bag-of-words (BoW) and tf-idf features as input. For the evaluation of your spam detector, an SMS collection is available as a dataset - this has yet to be suitably split into train and test data. To keep the costs as low as possible and to avoid problems with copyrights, your boss insists on a new development with Python.</p>\n",
        "\n",
        "<p>Include a short description of the data preprocessing steps, method, experiment design, hyper-parameters, and evaluation metric. Also, document your findings, drawbacks, and potential improvements.</p>\n",
        "\n",
        "<p>Note: You need to implement the bag-of-words (BoW) and tf-idf feature extractor from scratch. You can use existing python libraries for other tasks.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fad12eba",
      "metadata": {
        "id": "fad12eba"
      },
      "source": [
        "**Dataset and Resources**\n",
        "\n",
        "* SMS Spam Collection Dataset: https://archive.ics.uci.edu/dataset/228/sms+spam+collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "29f1378f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spam</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   spam                                            message\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load dataset\n",
        "data = pd.read_csv(sms_spam_collection, sep='\\t', header=None, names=['spam', 'message'])\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6473f44f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spam</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    spam                                            message\n",
              "0  False  Go until jurong point, crazy.. Available only ...\n",
              "1  False                      Ok lar... Joking wif u oni...\n",
              "2   True  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3  False  U dun say so early hor... U c already then say...\n",
              "4  False  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Change spam's values to boolean\n",
        "data['spam'] = data['spam'].map({'ham': False, 'spam': True})\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "49c42b63",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "spam\n",
              "False    4825\n",
              "True      747\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# View summary of 'spam' column\n",
        "data['spam'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4cd4c628",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['message'], data['spam'], test_size=0.2, random_state=42)\n",
        "\n",
        "train_df = pd.DataFrame({\n",
        "    'message': X_train,\n",
        "    'spam': y_train\n",
        "}).reset_index(drop=True)\n",
        "\n",
        "test_df = pd.DataFrame({\n",
        "    'message': X_test,\n",
        "    'spam': y_test\n",
        "}).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "aa4e1873",
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(corpus):\n",
        "    \"\"\"\n",
        "    Tokenise corpus into individual words\n",
        "    \"\"\"\n",
        "    words = re.split(r\"\\W+\", corpus) # spilt based on non-alphanumeric\n",
        "    words = [word.lower() for word in words if len(word) > 0] # lowercase words and ignore ''\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "20b5d663",
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_tokens(tokens):\n",
        "    \"\"\"Returns the count of tokens\"\"\"\n",
        "    return collections.Counter(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a727b84f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(corpus):\n",
        "    stop_words = stopwords.words('english')\n",
        "\n",
        "    words = tokenize(corpus)\n",
        "    words = [word for word in words if word not in stop_words] # remove stop words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b87099b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "class OrderedCounter(collections.Counter, collections.OrderedDict):\n",
        "    pass\n",
        "\n",
        "def bag_of_words(documents: pd.DataFrame):\n",
        "    spamC = OrderedCounter()\n",
        "    hamC = OrderedCounter()\n",
        "\n",
        "    \n",
        "\n",
        "    for row in documents.itertuples(index=False):\n",
        "        msg = preprocess(row.message)\n",
        "        spam = row.spam\n",
        "        \n",
        "        # Tag document to the corresponding Counter\n",
        "        if spam:\n",
        "            spamC.update(msg)\n",
        "        else:\n",
        "            hamC.update(msg)\n",
        "\n",
        "        \n",
        "    return spamC, hamC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "19a8fd0b",
      "metadata": {},
      "outputs": [
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/jiahuidai/nltk_data'\n    - '/Users/jiahuidai/Desktop/CAI_NatL - Spoken and Natural Language Understanding/Practical/.venv/nltk_data'\n    - '/Users/jiahuidai/Desktop/CAI_NatL - Spoken and Natural Language Understanding/Practical/.venv/share/nltk_data'\n    - '/Users/jiahuidai/Desktop/CAI_NatL - Spoken and Natural Language Understanding/Practical/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CAI_NatL - Spoken and Natural Language Understanding/Practical/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:84\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CAI_NatL - Spoken and Natural Language Understanding/Practical/.venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/jiahuidai/nltk_data'\n    - '/Users/jiahuidai/Desktop/CAI_NatL - Spoken and Natural Language Understanding/Practical/.venv/nltk_data'\n    - '/Users/jiahuidai/Desktop/CAI_NatL - Spoken and Natural Language Understanding/Practical/.venv/share/nltk_data'\n    - '/Users/jiahuidai/Desktop/CAI_NatL - Spoken and Natural Language Understanding/Practical/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m spamC, hamC = \u001b[43mbag_of_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mbag_of_words\u001b[39m\u001b[34m(documents)\u001b[39m\n\u001b[32m      6\u001b[39m hamC = OrderedCounter()\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m documents.itertuples(index=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     msg = \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     spam = row.spam\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Tag document to the corresponding Counter\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mpreprocess\u001b[39m\u001b[34m(corpus)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess\u001b[39m(corpus):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     stop_words = \u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m     words = tokenize(corpus)\n\u001b[32m      5\u001b[39m     words = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CAI_NatL - Spoken and Natural Language Understanding/Practical/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:120\u001b[39m, in \u001b[36mLazyCorpusLoader.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLazyCorpusLoader object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CAI_NatL - Spoken and Natural Language Understanding/Practical/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:86\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m             root = nltk.data.find(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.subdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[32m     89\u001b[39m corpus = \u001b[38;5;28mself\u001b[39m.__reader_cls(root, *\u001b[38;5;28mself\u001b[39m.__args, **\u001b[38;5;28mself\u001b[39m.__kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CAI_NatL - Spoken and Natural Language Understanding/Practical/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:81\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CAI_NatL - Spoken and Natural Language Understanding/Practical/.venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/jiahuidai/nltk_data'\n    - '/Users/jiahuidai/Desktop/CAI_NatL - Spoken and Natural Language Understanding/Practical/.venv/nltk_data'\n    - '/Users/jiahuidai/Desktop/CAI_NatL - Spoken and Natural Language Understanding/Practical/.venv/share/nltk_data'\n    - '/Users/jiahuidai/Desktop/CAI_NatL - Spoken and Natural Language Understanding/Practical/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "spamC, hamC = bag_of_words(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4fb9376",
      "metadata": {},
      "outputs": [],
      "source": [
        "hamC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b15f158",
      "metadata": {},
      "outputs": [],
      "source": [
        "spamC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4109920",
      "metadata": {
        "id": "f4109920"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def tf_idf(documents):\n",
        "    ## Complete the function\n",
        "    pass\n",
        "\n",
        "## You can use sklearn or other python libraries for naive bayes classifier, evaluation metric, etc.  ##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jEykmdVPwqPA",
      "metadata": {
        "id": "jEykmdVPwqPA"
      },
      "source": [
        " ### Search Engine *(40 points)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-CRJj4Ypw1Z2",
      "metadata": {
        "id": "-CRJj4Ypw1Z2"
      },
      "source": [
        "Your boss is impressed with your spam detector and assigns you a new task. As part of improving internal tools, the company wants a search engine that can search through SMS messages and rank them by relevance. Implement the PageRank algorithm from scratch to score each SMS message based on its importance in the document graph.\n",
        "\n",
        "*   Compute TF-IDF vectors for all SMS messages (you can leverage previous implementation)\n",
        "*   Construct a document graph, where each node represents an SMS message and edges are the links between nodes.\n",
        "*  Implement the PageRank algorithm from scratch to assign an importance score to each SMS message based on its position in the document graph.\n",
        "\n",
        "#### Hint : You can use the previous dataset or any dataset from your choice.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G_2IblEnUeju",
      "metadata": {
        "id": "G_2IblEnUeju"
      },
      "source": [
        "## You might need the follwoing formulas for your implementation\n",
        "\n",
        "---\n",
        "\n",
        "### 1) Cosine Similarity Between Two Document Vectors\n",
        "\n",
        "Cosine similarity measures how similar two vectors are based on the angle between them:\n",
        "\n",
        "$$\n",
        "\\text{cosine\\_sim}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\cdot \\|B\\|}\n",
        "$$\n",
        "\n",
        "- \\( A \\cdot B \\): Dot product of vectors \\( A \\) and \\( B \\)  \n",
        "- \\( \\|A\\| \\): Euclidean norm (magnitude) of vector \\( A \\)  \n",
        "- \\( \\|B\\| \\): Euclidean norm of vector \\( B \\)\n",
        "\n",
        "**Use case**: Comparing TF-IDF vectors to measure similarity between two messages.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) PageRank of a Node \\( i \\)\n",
        "\n",
        "PageRank estimates the importance of a document based on its connections in a graph:\n",
        "\n",
        "$$\n",
        "PR(i) = \\frac{1 - d}{N} + d \\sum_{j \\in M(i)} \\frac{PR(j)}{L(j)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( PR(i) \\): PageRank score of node \\( i \\)  \n",
        "- \\( d \\): Damping factor (typically 0.85)  \n",
        "- \\( N \\): Total number of nodes (documents) in the graph  \n",
        "- \\( M(i) \\): Set of nodes that link to node \\( i \\)  \n",
        "- \\( L(j) \\): Number of outbound links from node \\( j \\)  \n",
        "\n",
        "**Interpretation**:  \n",
        "- A document is important if **important documents link to it**.  \n",
        "- The score is split among a nodeâ€™s outbound links.  \n",
        "- The **teleportation term** $\\text(\\frac{1 - d}{N})$ accounts for random jumps, ensuring stability and fairness.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55cbc82a",
      "metadata": {
        "id": "55cbc82a"
      },
      "source": [
        "### Additional Experiments *(5 additional points - <span style=\"color: red;\">Optional</span>)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b5820d4",
      "metadata": {
        "id": "9b5820d4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
